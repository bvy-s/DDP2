{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98455947-acfe-458b-9b77-121887ddf4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING device : cuda\n",
      "NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"USING device :\",device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ef47b",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a17738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75aa300b-30b0-4811-9be9-65853f547d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r\"../datasets/office31\"\n",
    "num_domains = 3\n",
    "num_classes = 31\n",
    "all_domains = [\"amazon\", \"dslr\", \"webcam\"]\n",
    "all_classes = [\"back_pack\", \"bike\", \"bike_helmet\", \"bookcase\", \"bottle\", \"calculator\", \"desk_chair\", \"desk_lamp\", \\\n",
    "               \"desktop_computer\", \"file_cabinet\", \"headphones\", \"keyboard\", \"laptop_computer\", \"letter_tray\", \"mobile_phone\", \\\n",
    "                \"monitor\", \"mouse\", \"mug\", \"paper_notebook\", \"pen\", \"phone\", \"printer\", \"projector\", \"punchers\", \\\n",
    "                \"ring_binder\", \"ruler\", \"scissors\", \"speaker\", \"stapler\", \"tape_dispenser\", \"trash_can\"]\n",
    "\n",
    "train_dom_idx = [0,1]\n",
    "eval_dom_idx = [2]\n",
    "train_class_idx = list(range(29))\n",
    "eval_class_idx = [29,30]\n",
    "train_domains = all_domains[:2]\n",
    "eval_domains = all_domains[2:]\n",
    "train_classes = all_classes[:29]\n",
    "eval_classes = all_classes[29:]\n",
    "\n",
    "len(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f2c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reader(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8169d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTrain_images = []\n",
    "cTrain_images = {dom:[] for dom in train_domains}\n",
    "dTest_images = []\n",
    "cTest_images = {dom:[] for dom in train_domains}\n",
    "dTrain_label = []\n",
    "cTrain_label = {dom:[] for dom in train_domains}\n",
    "dTest_label = []\n",
    "cTest_label = {dom:[] for dom in train_domains}\n",
    "\n",
    "for d in range(len(train_domains)):\n",
    "    domain = train_domains[d]\n",
    "    for c in range(len(train_classes)):\n",
    "        clas = train_classes[c]\n",
    "        directory_path = os.path.join(data_path, domain, clas)\n",
    "        for file_name in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            if np.random.uniform(0.0, 1.0) < 0.8:\n",
    "                dTrain_images.append(image_reader(file_path))\n",
    "                dTrain_label.append(d)\n",
    "                cTrain_images[domain].append(image_reader(file_path))\n",
    "                cTrain_label[domain].append(c)\n",
    "            else:\n",
    "                dTest_images.append(image_reader(file_path))\n",
    "                dTest_label.append(d)\n",
    "                cTest_images[domain].append(image_reader(file_path))\n",
    "                cTest_label[domain].append(c)\n",
    "\n",
    "dLabel_mapping = {i:train_domains[i] for i in range(len(train_domains))}\n",
    "cLabel_mapping = {i:train_classes[i] for i in range(len(train_classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "753cf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpi_shuffler(images, labels):\n",
    "    images_np = np.array(images)\n",
    "    encoded_labels_np = np.array(labels)\n",
    "\n",
    "    # Create a random permutation of indices and shuffle arrays\n",
    "    indices = np.random.permutation(len(images_np))\n",
    "    shuffled_images = images_np[indices]\n",
    "    shuffled_labels = encoded_labels_np[indices]\n",
    "\n",
    "    return shuffled_images, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f082aa-1a61-4127-a3b6-b2fd0ab31396",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTrain_images, dTrain_label = numpi_shuffler(dTrain_images, dTrain_label)\n",
    "dTest_images, dTest_label = numpi_shuffler(dTest_images, dTest_label)\n",
    "for dom in train_domains:\n",
    "    cTrain_images[dom], cTrain_label[dom] = numpi_shuffler(cTrain_images[dom], cTrain_label[dom])\n",
    "    cTest_images[dom], cTest_label[dom] = numpi_shuffler(cTest_images[dom], cTest_label[dom])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf82efb",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8553e87-fd90-4f92-81dd-82a382d77e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6451e3bc-6c18-4c70-a3e8-70272f35da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Custom Dataset with Data Augmentation ----------------\n",
    "# Assuming training_images and testing_images are NumPy arrays with shape (N, 128, 128, 3)\n",
    "# and training_labels and testing_labels are NumPy arrays with integer labels.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        images: NumPy array of shape (N, 128, 128, 3)\n",
    "        labels: NumPy array of shape (N,)\n",
    "        transform: torchvision transforms to be applied on each image\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Convert the NumPy image (H, W, C) to a PIL Image for transforms\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))  # if images are in [0,1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad76e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (32, 128, 128)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (32, 64, 64)\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # (64, 64, 64)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (64, 32, 32)\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (128, 32, 32)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)          # (128, 16, 16)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the feature maps: alternative to x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c2a2c8a-b7e6-40e9-977b-ddd26a221a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Define Transforms ----------------\n",
    "# Training transform with data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # Rotate by up to 10 degrees\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),  # Random crop and resize back to 128x128\n",
    "    transforms.ToTensor(),\n",
    "    # If you used normalization during training, add it here.\n",
    "    # e.g., transforms.Normalize(mean=[...], std=[...])\n",
    "])\n",
    "\n",
    "# Testing transform: only resize and convert to tensor\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961262c6-8e4f-46de-a52f-4f7068446ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom datasets and Dataloaders\n",
    "dTrain_dataset = CustomDataset(dTrain_images, dTrain_label, transform=train_transform)\n",
    "dTest_dataset = CustomDataset(dTest_images, dTest_label, transform=test_transform)\n",
    "dTrain_loader = DataLoader(dTrain_dataset, batch_size=32, shuffle=True)\n",
    "dTest_loader  = DataLoader(dTest_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "cTrain_datasets = {dom:CustomDataset(cTrain_images[dom], cTrain_label[dom], transform=train_transform) for dom in train_domains}\n",
    "cTest_datasets = {dom:CustomDataset(cTest_images[dom], cTest_label[dom], transform=train_transform) for dom in train_domains}\n",
    "cTrain_loaders = {dom:DataLoader(cTrain_datasets[dom], batch_size=32, shuffle=True) for dom in train_domains}\n",
    "cTest_loaders  = {dom:DataLoader(cTest_datasets[dom], batch_size=32, shuffle=False) for dom in train_domains}\n",
    "\n",
    "domain_model = SimpleCNN(num_classes=len(train_domains)).to(device)\n",
    "class_models = {dom:SimpleCNN(num_classes=len(train_classes)).to(device) for dom in train_domains}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d230b51-46c3-4542-8385-008cae1080d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "def Trainer(num_epochs, learning_rate, weight_d, model, train_loader, train_dataset):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_d)\n",
    "    # Optionally, add a learning rate scheduler:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images=images.to(device)\n",
    "            #labels = torch.tensor(labels)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Update scheduler if using one\n",
    "        scheduler.step(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b2f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon Classifier\n",
      "Epoch [1/50], Loss: 3.3605\n",
      "Epoch [2/50], Loss: 3.2357\n",
      "Epoch [3/50], Loss: 3.0911\n",
      "Epoch [4/50], Loss: 3.0115\n",
      "Epoch [5/50], Loss: 2.9216\n",
      "Epoch [6/50], Loss: 2.8534\n",
      "Epoch [7/50], Loss: 2.7990\n",
      "Epoch [8/50], Loss: 2.7261\n",
      "Epoch [9/50], Loss: 2.6629\n",
      "Epoch [10/50], Loss: 2.6157\n",
      "Epoch [11/50], Loss: 2.5822\n",
      "Epoch [12/50], Loss: 2.5280\n",
      "Epoch [13/50], Loss: 2.4896\n",
      "Epoch [14/50], Loss: 2.4447\n",
      "Epoch [15/50], Loss: 2.3892\n",
      "Epoch [16/50], Loss: 2.3508\n",
      "Epoch [17/50], Loss: 2.3261\n",
      "Epoch [18/50], Loss: 2.3065\n",
      "Epoch [19/50], Loss: 2.2535\n",
      "Epoch [20/50], Loss: 2.1914\n",
      "Epoch [21/50], Loss: 2.1828\n",
      "Epoch [22/50], Loss: 2.1114\n",
      "Epoch [23/50], Loss: 2.1330\n",
      "Epoch [24/50], Loss: 2.1281\n",
      "Epoch [25/50], Loss: 2.0701\n",
      "Epoch [26/50], Loss: 2.0365\n",
      "Epoch [27/50], Loss: 1.9917\n",
      "Epoch [28/50], Loss: 2.0152\n",
      "Epoch [29/50], Loss: 1.9566\n",
      "Epoch [30/50], Loss: 1.9564\n",
      "Epoch [31/50], Loss: 1.9228\n",
      "Epoch [32/50], Loss: 1.9503\n",
      "Epoch [33/50], Loss: 1.8979\n",
      "Epoch [34/50], Loss: 1.9058\n",
      "Epoch [35/50], Loss: 1.8558\n",
      "Epoch [36/50], Loss: 1.8184\n",
      "Epoch [37/50], Loss: 1.8237\n",
      "Epoch [38/50], Loss: 1.8039\n",
      "Epoch [39/50], Loss: 1.8020\n",
      "Epoch [40/50], Loss: 1.8094\n",
      "Epoch [41/50], Loss: 1.7677\n",
      "Epoch [42/50], Loss: 1.7413\n",
      "Epoch [43/50], Loss: 1.7202\n",
      "Epoch [44/50], Loss: 1.7591\n",
      "Epoch [45/50], Loss: 1.7174\n",
      "Epoch [46/50], Loss: 1.7210\n",
      "Epoch [47/50], Loss: 1.6768\n",
      "Epoch [48/50], Loss: 1.6629\n",
      "Epoch [49/50], Loss: 1.6267\n",
      "Epoch [50/50], Loss: 1.6581\n",
      "dslr Classifier\n",
      "Epoch [1/50], Loss: 3.3643\n",
      "Epoch [2/50], Loss: 3.3320\n",
      "Epoch [3/50], Loss: 3.3091\n",
      "Epoch [4/50], Loss: 3.2716\n",
      "Epoch [5/50], Loss: 3.2138\n",
      "Epoch [6/50], Loss: 3.1770\n",
      "Epoch [7/50], Loss: 3.0897\n",
      "Epoch [8/50], Loss: 2.9748\n",
      "Epoch [9/50], Loss: 2.8374\n",
      "Epoch [10/50], Loss: 2.6906\n",
      "Epoch [11/50], Loss: 2.5600\n",
      "Epoch [12/50], Loss: 2.4235\n",
      "Epoch [13/50], Loss: 2.3430\n",
      "Epoch [14/50], Loss: 2.1479\n",
      "Epoch [15/50], Loss: 2.1238\n",
      "Epoch [16/50], Loss: 2.0359\n",
      "Epoch [17/50], Loss: 1.9639\n",
      "Epoch [18/50], Loss: 1.9333\n",
      "Epoch [19/50], Loss: 1.8362\n",
      "Epoch [20/50], Loss: 1.7525\n",
      "Epoch [21/50], Loss: 1.6379\n",
      "Epoch [22/50], Loss: 1.6183\n",
      "Epoch [23/50], Loss: 1.6087\n",
      "Epoch [24/50], Loss: 1.5744\n",
      "Epoch [25/50], Loss: 1.5166\n",
      "Epoch [26/50], Loss: 1.3987\n",
      "Epoch [27/50], Loss: 1.3408\n",
      "Epoch [28/50], Loss: 1.3338\n",
      "Epoch [29/50], Loss: 1.3549\n",
      "Epoch [30/50], Loss: 1.2528\n",
      "Epoch [31/50], Loss: 1.2302\n",
      "Epoch [32/50], Loss: 1.1970\n",
      "Epoch [33/50], Loss: 1.2011\n",
      "Epoch [34/50], Loss: 1.1256\n",
      "Epoch [35/50], Loss: 1.1083\n",
      "Epoch [36/50], Loss: 1.0650\n",
      "Epoch [37/50], Loss: 1.0076\n",
      "Epoch [38/50], Loss: 0.9588\n",
      "Epoch [39/50], Loss: 0.9578\n",
      "Epoch [40/50], Loss: 0.9112\n",
      "Epoch [41/50], Loss: 0.9948\n",
      "Epoch [42/50], Loss: 0.8949\n",
      "Epoch [43/50], Loss: 0.8942\n",
      "Epoch [44/50], Loss: 0.9176\n",
      "Epoch [45/50], Loss: 0.8272\n",
      "Epoch [46/50], Loss: 0.8717\n",
      "Epoch [47/50], Loss: 0.8021\n",
      "Epoch [48/50], Loss: 0.7805\n",
      "Epoch [49/50], Loss: 0.8223\n",
      "Epoch [50/50], Loss: 0.7928\n"
     ]
    }
   ],
   "source": [
    "# print(\"Domain Classifier\")\n",
    "# Trainer(num_epochs, 0.001, 1e-5, domain_model, dTrain_loader, dTrain_dataset)\n",
    "\n",
    "for dom in train_domains:\n",
    "    print(dom+\" Classifier\")\n",
    "    Trainer(50, 0.0001, 1e-5, class_models[dom], cTrain_loaders[dom], cTrain_datasets[dom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73755dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classifier\n",
      "Epoch [1/20], Loss: 0.1055\n",
      "Epoch [2/20], Loss: 0.0566\n",
      "Epoch [3/20], Loss: 0.0409\n",
      "Epoch [4/20], Loss: 0.0351\n",
      "Epoch [5/20], Loss: 0.0198\n",
      "Epoch [6/20], Loss: 0.0318\n",
      "Epoch [7/20], Loss: 0.0241\n",
      "Epoch [8/20], Loss: 0.0355\n",
      "Epoch [9/20], Loss: 0.0141\n",
      "Epoch [10/20], Loss: 0.0151\n",
      "Epoch [11/20], Loss: 0.0228\n",
      "Epoch [12/20], Loss: 0.0159\n",
      "Epoch [13/20], Loss: 0.0226\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [14/20], Loss: 0.0184\n",
      "Epoch [15/20], Loss: 0.0090\n",
      "Epoch [16/20], Loss: 0.0061\n",
      "Epoch [17/20], Loss: 0.0053\n",
      "Epoch [18/20], Loss: 0.0078\n",
      "Epoch [19/20], Loss: 0.0083\n",
      "Epoch [20/20], Loss: 0.0197\n"
     ]
    }
   ],
   "source": [
    "print(\"Domain Classifier\")\n",
    "Trainer(num_epochs, 0.001, 1e-5, domain_model, dTrain_loader, dTrain_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47da10-2c5b-4acb-9154-f64584b838ff",
   "metadata": {},
   "source": [
    "***MODEL EVALUATION**\n",
    "\n",
    "1. CLASSIFICATION REPORT USING SEG_TEST\n",
    "2. CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbfecf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d3d4289-d9ef-4ac6-b00c-b0fd2ec8aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tester(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_preds=[]\n",
    "    all_labels=[]\n",
    "    all_certs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images,labels in test_loader:\n",
    "            images=images.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            certs,preds = torch.max(probs,1)\n",
    "            all_certs.extend(certs.cpu().numpy())\n",
    "            #_,preds=torch.max(outputs,1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds=np.array(all_preds)\n",
    "    all_labels=np.array(all_labels)\n",
    "    all_certs=np.array(all_certs)\n",
    "\n",
    "    report= classification_report(all_labels,all_preds,target_names=class_names)\n",
    "    print(\"classification report:\\n\",report)\n",
    "\n",
    "    conf_matrix= confusion_matrix(all_labels,all_preds)\n",
    "    print(\"Confusion Matrix:\\n\",conf_matrix)\n",
    "\n",
    "    # print(outputs)\n",
    "    # print(probs)\n",
    "    # print(preds)\n",
    "\n",
    "    # plt.figure(figsize=(8,6))\n",
    "    # sns.heatmap(conf_matrix,annot=True,fmt=\"d\",cmap=\"Blues\",xticklabels=class_names,yticklabels=class_names)\n",
    "    # plt.xlabel(\"predicted class\")\n",
    "    # plt.ylabel(\"actual class\")\n",
    "    # plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.hist(all_certs, 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e6e2914-57af-423a-bbd2-397a4a5285a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      amazon       1.00      0.99      1.00       534\n",
      "        dslr       0.97      1.00      0.98       112\n",
      "\n",
      "    accuracy                           0.99       646\n",
      "   macro avg       0.98      1.00      0.99       646\n",
      "weighted avg       0.99      0.99      0.99       646\n",
      "\n",
      "Confusion Matrix:\n",
      " [[530   4]\n",
      " [  0 112]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tester(domain_model, dTest_loader, train_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66c13149-ef27-452a-bf69-4290ea1bb410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       back_pack       0.65      0.76      0.70        17\n",
      "            bike       0.75      1.00      0.86        12\n",
      "     bike_helmet       0.37      0.50      0.43        20\n",
      "        bookcase       0.58      0.39      0.47        18\n",
      "          bottle       0.00      0.00      0.00         4\n",
      "      calculator       0.36      0.62      0.45         8\n",
      "      desk_chair       0.71      0.80      0.75        15\n",
      "       desk_lamp       0.53      0.57      0.55        14\n",
      "desktop_computer       0.79      0.37      0.50        30\n",
      "    file_cabinet       0.37      0.50      0.43        20\n",
      "      headphones       0.62      0.64      0.63        25\n",
      "        keyboard       0.50      0.58      0.54        26\n",
      " laptop_computer       0.61      0.70      0.65        20\n",
      "     letter_tray       0.44      0.46      0.45        24\n",
      "    mobile_phone       0.62      0.53      0.57        15\n",
      "         monitor       0.91      0.78      0.84        27\n",
      "           mouse       0.46      0.35      0.40        17\n",
      "             mug       0.45      0.64      0.53        14\n",
      "  paper_notebook       0.53      0.50      0.52        16\n",
      "             pen       0.39      0.37      0.38        19\n",
      "           phone       0.29      0.40      0.33        20\n",
      "         printer       0.53      0.38      0.44        26\n",
      "       projector       0.50      0.47      0.49        19\n",
      "        punchers       0.07      0.07      0.07        15\n",
      "     ring_binder       0.29      0.27      0.28        15\n",
      "           ruler       0.53      0.67      0.59        12\n",
      "        scissors       0.57      0.57      0.57        21\n",
      "         speaker       0.68      0.52      0.59        25\n",
      "         stapler       0.43      0.30      0.35        20\n",
      "\n",
      "        accuracy                           0.51       534\n",
      "       macro avg       0.50      0.51      0.49       534\n",
      "    weighted avg       0.53      0.51      0.51       534\n",
      "\n",
      "Confusion Matrix:\n",
      " [[13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  2  0  0  0\n",
      "   0  0  0  1  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0 10  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  1  2  1  1  0  2\n",
      "   0  0  0  1  0]\n",
      " [ 0  0  0  7  0  1  0  1  0  3  3  0  0  0  1  0  0  0  0  0  0  0  1  0\n",
      "   1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  1  1  1  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0  0  0  1  0  1  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0 12  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  8  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  1\n",
      "   1  1  0  0  0]\n",
      " [ 2  0  2  0  0  1  0  0 11  0  0  0  3  1  3  0  2  0  0  1  2  0  0  0\n",
      "   0  0  0  2  0]\n",
      " [ 1  0  1  0  0  0  1  0  0 10  0  0  0  1  0  0  2  1  0  1  1  0  0  0\n",
      "   1  0  0  0  0]\n",
      " [ 0  1  1  1  0  1  1  0  0  0 16  1  0  1  0  0  1  1  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1 15  1  2  0  0  0  1  1  0  0  1  2  0\n",
      "   0  1  0  0  1]\n",
      " [ 0  0  1  0  0  0  0  0  1  1  0  0 14  0  0  0  0  0  0  0  1  1  0  0\n",
      "   1  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  0  0  0  0  0  1  0 11  0  0  0  1  0  0  2  2  0  2\n",
      "   1  0  0  0  2]\n",
      " [ 0  0  2  1  0  2  0  1  0  0  1  0  0  0  8  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  1  0  1  0  0  0  1  0  0 21  0  1  0  0  0  1  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  1  0  0  1  1  0  1  0  0  6  1  1  0  2  1  0  1\n",
      "   0  0  0  0  0]\n",
      " [ 1  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0\n",
      "   1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  2  0  0  0  0  0  0  0  1  8  0  0  0  0  0\n",
      "   0  0  2  2  0]\n",
      " [ 0  0  1  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  7  1  1  0  0\n",
      "   1  1  4  0  1]\n",
      " [ 0  0  1  0  0  0  0  0  0  3  0  3  1  0  0  1  0  1  0  0  8  0  0  2\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  3  1  0  0  0  0  0  2  0  1  0  1  0  0  0  0  0  1  2 10  1  1\n",
      "   1  0  1  0  1]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  2  0  2  0  0  0  1  0  2  0  1  9  0\n",
      "   0  0  0  0  1]\n",
      " [ 0  0  2  0  0  0  0  0  0  2  1  3  1  0  0  0  0  0  0  1  0  0  2  1\n",
      "   0  0  1  0  1]\n",
      " [ 0  1  1  0  0  1  0  0  1  2  1  0  0  1  0  1  0  0  0  0  1  0  0  1\n",
      "   4  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  8  1  0  0]\n",
      " [ 0  0  0  1  1  0  0  0  0  1  0  1  0  0  0  0  0  0  0  1  0  0  0  3\n",
      "   0  1 12  0  0]\n",
      " [ 0  0  0  1  0  1  0  0  0  0  0  1  0  0  0  0  1  0  0  1  1  0  2  0\n",
      "   2  1  0 13  1]\n",
      " [ 0  0  1  0  1  0  1  0  0  0  0  1  1  3  0  0  0  0  3  0  1  0  1  0\n",
      "   0  1  0  0  6]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tester(class_models[\"amazon\"], cTest_loaders[\"amazon\"], train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "026bf0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e97af68d-d532-47d2-b921-8deba7daf508",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 28, does not match size of target_names, 29. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_604547/823146579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dslr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcTest_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dslr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_604547/1852415195.py\u001b[0m in \u001b[0;36mTester\u001b[0;34m(model, test_loader, class_names)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mall_certs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_certs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"classification report:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2133\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2135\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2136\u001b[0m             )\n\u001b[1;32m   2137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 28, does not match size of target_names, 29. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "Tester(class_models[\"dslr\"], cTest_loaders[\"dslr\"], train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc26603",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = \"../datasets/office31/webcam/bike/frame_0001.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model and image tensor to the device (GPU if available)\n",
    "def Evaluator(model, img_path, labels):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "        transforms.ToTensor(),          \n",
    "    ])\n",
    "\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    model.to(device)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    # Evaluate the model on the image\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #AttributeError: 'CustomDataset' object has no attribute 'size'\n",
    "        output = model(img_tensor)\n",
    "        print(type(output), output)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        print(probs)\n",
    "        _, predicted = torch.max(output)\n",
    "\n",
    "    predicted_label = predicted.item()\n",
    "    print(predicted_label, labels[predicted_label])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model and image tensor to the device (GPU if available)\n",
    "def CompleteEvaluator(domain_model, class_models, img_tensor, train_domains, train_classes):\n",
    "    domain_model.to(device)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    # Evaluate the domain model on the image\n",
    "    domain_model.eval()\n",
    "    with torch.no_grad():\n",
    "        #AttributeError: 'CustomDataset' object has no attribute 'size'\n",
    "        domain_logits = domain_model(img_tensor)\n",
    "        print(type(output), output)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    predicted_label = predicted.item()\n",
    "    print(predicted_label, labels[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([[0.0017, 0.1803]], device='cuda:0')\n",
      "tensor([[0.4555, 0.5445]], device='cuda:0')\n",
      "1 dslr\n"
     ]
    }
   ],
   "source": [
    "Evaluator(domain_model, pred_path, train_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6518c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([[-1.6326, -2.8096, -1.0975, -2.1476, -0.9902, -3.5406, -5.9390,  0.2154,\n",
      "         -0.2974, -0.2006, -1.3874, -2.7867, -2.8676, -1.9978, -2.6757, -3.6056,\n",
      "         -0.0799, -0.5774, -0.4301, -0.5248,  0.1537, -3.4979, -4.4339,  1.0425,\n",
      "         -0.9316, -1.7128, -0.6711, -0.7449, -2.5976]], device='cuda:0')\n",
      "tensor([[1.5129e-02, 4.6628e-03, 2.5836e-02, 9.0398e-03, 2.8764e-02, 2.2450e-03,\n",
      "         2.0398e-04, 9.6029e-02, 5.7506e-02, 6.3351e-02, 1.9333e-02, 4.7711e-03,\n",
      "         4.4003e-03, 1.0501e-02, 5.3311e-03, 2.1036e-03, 7.1475e-02, 4.3462e-02,\n",
      "         5.0361e-02, 4.5809e-02, 9.0283e-02, 2.3429e-03, 9.1883e-04, 2.1958e-01,\n",
      "         3.0498e-02, 1.3965e-02, 3.9574e-02, 3.6757e-02, 5.7643e-03]],\n",
      "       device='cuda:0')\n",
      "23 punchers\n"
     ]
    }
   ],
   "source": [
    "Evaluator(class_models[\"amazon\"], img_tensor, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da24e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([[ 0.4455,  5.0423, -3.3687,  0.4425, -5.1732, -2.7109,  4.9851, -2.2462,\n",
      "         -0.7672,  3.9686, -1.5845, -0.6298, -0.7301, -1.5858, -2.7337, -3.6440,\n",
      "         -0.3399, -2.1625, -2.1451, -0.9787, -1.1209,  0.2564, -1.6166, -0.8977,\n",
      "         -2.3688, -3.3043,  0.4179,  1.2234,  0.3381]], device='cuda:0')\n",
      "tensor([[4.2280e-03, 4.1931e-01, 9.3249e-05, 4.2156e-03, 1.5345e-05, 1.8003e-04,\n",
      "         3.9597e-01, 2.8652e-04, 1.2575e-03, 1.4328e-01, 5.5527e-04, 1.4426e-03,\n",
      "         1.3049e-03, 5.5457e-04, 1.7597e-04, 7.0807e-05, 1.9277e-03, 3.1153e-04,\n",
      "         3.1700e-04, 1.0177e-03, 8.8281e-04, 3.4997e-03, 5.3777e-04, 1.1036e-03,\n",
      "         2.5347e-04, 9.9458e-05, 4.1128e-03, 9.2040e-03, 3.7976e-03]],\n",
      "       device='cuda:0')\n",
      "1 bike\n"
     ]
    }
   ],
   "source": [
    "Evaluator(class_models[\"dslr\"], img_tensor, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfffcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfedc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_logits = [0.0017, 0.1803]\n",
    "dom_probs = [0.4555, 0.5445]\n",
    "\n",
    "amazon_logits = np.array([-1.6326, -2.8096, -1.0975, -2.1476, -0.9902, -3.5406, -5.9390,  0.2154,\n",
    "         -0.2974, -0.2006, -1.3874, -2.7867, -2.8676, -1.9978, -2.6757, -3.6056,\n",
    "         -0.0799, -0.5774, -0.4301, -0.5248,  0.1537, -3.4979, -4.4339,  1.0425,\n",
    "         -0.9316, -1.7128, -0.6711, -0.7449, -2.5976])\n",
    "amazon_probs = np.array([1.5129e-02, 4.6628e-03, 2.5836e-02, 9.0398e-03, 2.8764e-02, 2.2450e-03,\n",
    "         2.0398e-04, 9.6029e-02, 5.7506e-02, 6.3351e-02, 1.9333e-02, 4.7711e-03,\n",
    "         4.4003e-03, 1.0501e-02, 5.3311e-03, 2.1036e-03, 7.1475e-02, 4.3462e-02,\n",
    "         5.0361e-02, 4.5809e-02, 9.0283e-02, 2.3429e-03, 9.1883e-04, 2.1958e-01,\n",
    "         3.0498e-02, 1.3965e-02, 3.9574e-02, 3.6757e-02, 5.7643e-03])\n",
    "\n",
    "dslr_logits = np.array([ 0.4455,  5.0423, -3.3687,  0.4425, -5.1732, -2.7109,  4.9851, -2.2462,\n",
    "         -0.7672,  3.9686, -1.5845, -0.6298, -0.7301, -1.5858, -2.7337, -3.6440,\n",
    "         -0.3399, -2.1625, -2.1451, -0.9787, -1.1209,  0.2564, -1.6166, -0.8977,\n",
    "         -2.3688, -3.3043,  0.4179,  1.2234,  0.3381])\n",
    "dslr_probs = np.array([4.2280e-03, 4.1931e-01, 9.3249e-05, 4.2156e-03, 1.5345e-05, 1.8003e-04,\n",
    "         3.9597e-01, 2.8652e-04, 1.2575e-03, 1.4328e-01, 5.5527e-04, 1.4426e-03,\n",
    "         1.3049e-03, 5.5457e-04, 1.7597e-04, 7.0807e-05, 1.9277e-03, 3.1153e-04,\n",
    "         3.1700e-04, 1.0177e-03, 8.8281e-04, 3.4997e-03, 5.3777e-04, 1.1036e-03,\n",
    "         2.5347e-04, 9.9458e-05, 4.1128e-03, 9.2040e-03, 3.7976e-03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a3d3e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(dslr_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f0d9667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(0.0017*amazon_logits + 0.1803*dslr_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd449d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00919341, 0.2304382 , 0.01181907, 0.00641302, 0.01311036,\n",
       "       0.00112062, 0.21569858, 0.04389722, 0.02687869, 0.10687234,\n",
       "       0.00910853, 0.00295873, 0.00271485, 0.00508517, 0.00252413,\n",
       "       0.00099674, 0.0336065 , 0.01996657, 0.02311204, 0.02142014,\n",
       "       0.0416046 , 0.00297278, 0.00071134, 0.1006196 , 0.01402985,\n",
       "       0.00641521, 0.02026538, 0.02175439, 0.00469343])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4555*amazon_probs + 0.5445*dslr_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3227c820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(0.0017*amazon_logits + 0.1803*dslr_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91628065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274f753-9db7-4e7a-9b25-7109e23604c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\\\n",
    "# # Path to your prediction folder\n",
    "pred_folder = \"Downloads/archive/seg_pred/seg_pred\"\n",
    "\n",
    "# List all image files in the folder (adjust the extensions as needed)\n",
    "pred_files = [os.path.join(pred_folder, f) for f in os.listdir(pred_folder)\n",
    "               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# # Choose a random image from the folder\n",
    "random_image_path = random.choice(pred_files)\n",
    "print(\"Random image path:\", random_image_path)\n",
    "\n",
    "# Open the image and ensure it's in RGB format\n",
    "img = Image.open(random_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define the same transformation used in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),          # Converts to tensor and scales pixel values to [0, 1]\n",
    "    # If you applied normalization during training, add it here.\n",
    "    # Example: transforms.Normalize(mean=[...], std=[...])\n",
    "])\n",
    "\n",
    "# Apply transformation and add batch dimension\n",
    "img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# Move model and image tensor to the device (GPU if available)\n",
    "model.to(device)\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Evaluate the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():AttributeError: 'CustomDataset' object has no attribute 'size'\n",
    "    output = model(img_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "predicted_label = predicted.item()\n",
    "\n",
    "# Define the class names in the same order as during training\n",
    "class_names = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "predicted_class_name = class_names[predicted_label]\n",
    "\n",
    "print(\"Predicted label:\", predicted_label)\n",
    "print(\"Predicted class:\", predicted_class_name)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Predicted label: {predicted_label} - {predicted_class_name}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89b40d-2efa-4094-81e8-1239dcf668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (32, 128, 128)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (32, 64, 64)\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (64, 32, 32)\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)          # (128, 16, 16)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the feature maps: alternative to x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = ImprovedCNN(num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1072b-0b4a-4ca4-9b24-e70bf508ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "k_folds = 10\n",
    "num_epochs = 20\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(train_dataset)) \n",
    "\n",
    "# Create a dictionary to record validation losses for each fold across epochs.\n",
    "fold_val_losses = {i: [] for i in range(k_folds)}\n",
    "\n",
    "\n",
    "model = ImprovedCNN(num_classes=6).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    current_fold = epoch % k_folds  \n",
    "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs}, Using Fold {current_fold+1} as Validation ---\")\n",
    "    \n",
    "    train_idx, val_idx = folds[current_fold]\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()           \n",
    "        outputs = model(images)           \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()                   \n",
    "        optimizer.step()                  \n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    # Compute average training loss for this epoch\n",
    "    train_loss = running_loss / len(train_subset)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    val_loss /= len(val_subset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Record the validation loss for the current fold\n",
    "    fold_val_losses[current_fold].append(val_loss)\n",
    "\n",
    "# After training, compute the average validation loss for each fold.\n",
    "avg_fold_losses = {fold: np.mean(losses) for fold, losses in fold_val_losses.items()}\n",
    "print(\"\\nAverage Validation Loss per Fold:\")\n",
    "for fold in sorted(avg_fold_losses.keys()):\n",
    "    print(f\"Fold {fold+1}: {avg_fold_losses[fold]:.4f}\")\n",
    "\n",
    "# Then, compute the overall average validation loss across folds.\n",
    "overall_avg_val_loss = np.mean(list(avg_fold_losses.values()))\n",
    "print(f\"\\nOverall Average Validation Loss across folds: {overall_avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50f203-6d68-4584-9811-b56a07149c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names= [\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"]\n",
    "\n",
    "model.eval()\n",
    "all_preds=[]\n",
    "all_labels=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images,labels in test_loader:\n",
    "        images=images.to(device)\n",
    "        labels= labels.to(device)\n",
    "        outputs= model(images)\n",
    "        _,preds=torch.max(outputs,1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds=np.array(all_preds)\n",
    "all_labels=np.array(all_labels)\n",
    "\n",
    "report= classification_report(all_labels,all_preds,target_names=class_names)\n",
    "print(\"classification report:\\n\",report)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bd6bc-3ce3-49ea-a06f-97a9a4db79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix= confusion_matrix(all_labels,all_preds)\n",
    "print(\"Confusion Matrix:\\n\",conf_matrix)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix,annot=True,fmt=\"d\",cmap=\"Blues\",xticklabels=class_names,yticklabels=class_names)\n",
    "plt.xlabel(\"predicted class\")\n",
    "plt.ylabel(\"actual class\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9210a-2194-4a44-82cb-4aaea08d3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\\\n",
    "# # Path to your prediction folder\n",
    "pred_folder = \"Downloads/archive/seg_pred/seg_pred\"\n",
    "\n",
    "# List all image files in the folder (adjust the extensions as needed)\n",
    "pred_files = [os.path.join(pred_folder, f) for f in os.listdir(pred_folder)\n",
    "               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# # Choose a random image from the folder\n",
    "random_image_path = random.choice(pred_files)\n",
    "print(\"Random image path:\", random_image_path)\n",
    "\n",
    "# Open the image and ensure it's in RGB format\n",
    "img = Image.open(random_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define the same transformation used in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),          # Converts to tensor and scales pixel values to [0, 1]\n",
    "    # If you applied normalization during training, add it here.\n",
    "    # Example: transforms.Normalize(mean=[...], std=[...])\n",
    "])\n",
    "\n",
    "\n",
    "# Apply transformation and add batch dimension\n",
    "img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# Move model and image tensor to the device (GPU if available)\n",
    "model.to(device)\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Evaluate the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "predicted_label = predicted.item()\n",
    "\n",
    "# Define the class names in the same order as during training\n",
    "class_names = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "predicted_class_name = class_names[predicted_label]\n",
    "\n",
    "print(\"Predicted label:\", predicted_label)\n",
    "print(\"Predicted class:\", predicted_class_name)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Predicted label: {predicted_label} - {predicted_class_name}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a172da-8f31-4953-b510-da2db5672da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
